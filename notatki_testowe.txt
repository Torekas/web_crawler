różne metody rozszerzania wiedzy i radzenia sobie z noisem.
w Dragan i Donadello są struktury jak tworzyć knowledge graphy TSL NLE lub wymienione NLG (natural language generation) (szukanie wiedzy online lub od usera).

judge model - deterministyczny = temperature = 0.0
gen model - twórczy => temp > 0.0 np. 0.3

datasety:
Arc - trudny dataset, pytanie wielokrotnego wyboru, dziedziny ścisłe
GSM8K - pytania tekstowe, matematyka + fizyka
truthfulqa - pytania podchwytliwe, podatne na halucynacje
SQUAD - reading comprehension, context QA, model ma wyciągnąć odp. z kontekstu
MMLU history - trudny test wiedzy szkolnej
szkodliwe pytania - jak ukraść auto itp => bezpieczeństwo + jailbreak

multimodal tak jak w langfuse
biblioteki

Co to są GEval i Opik?
GEval - generator metryk oceniających.
tworzenie metryk opisowych np. GEval Commons - correctness, basic instruction = you evaluate correctness, evaluation criteria = factual correctness 0-1 i model = "judge model".

DSPY - traktuje LLM-y jako model algorytmiczny.
- automatycznie optymalizuje prompt’y i przykłady (few-shot)
- uczy model jak myśleć, a nie tylko co wygenerować
- współpraca CoT, tool usage, submoduły, automatyczny bootstrapping

LLM programming + Automatic prompt optimization
Tak trudne jak pytorch
- oddziela logikę od promptów
- MiProV2 to automatyczny optymalizator promptów (np algorytm genetyczny lub bayesowski):
  * generuje różne style przykładów few-shot
  * porównuje inne prompty i porównuje je z metryką
  * wybiera najlepszą kombinację promptów + przykład
  * działa też jak model inference
  * sam buduje najlepsze prompty
- generuje setki promptów, różne few-shot examples, testuje je na datasetach i wybiera najlepszą opcję
- oddziela co robi funkcja od tego jak prompt ją realizuje

pomysły:
- próba nauki modeli na historiach konwersacji i swoich błedach
- próba wprowadzenia chatbot areny (multi-agent reflexion)
- iteracyjne budowanie synthetic datasetu:
  model robi dataset -> SFT1 -> model działa lepiej, i gen lepsze dane -> ...
  meta-model przewiduje wagę opinii dla każdego judge LLM bardziej kompetentny dodaje większą pewność

- metryka sprawdzająca reasoning.
- stworzenie „sądu” - prokurator gnębie model, obrońca mówi dlaczego model dobrze odpowiedział., sędzia decyduje.
nauka modelu zamiast w architekturę Alpaca, czyli instrukcje - odpowiedz to cały proces myślowy <think ... <think>

- a co jeśli nie mamy „golden datasetu”? podejścia zero-shot:
  - generujemy kilka odp. na to samo pytanie z różnymi „maskami”, naukowiec pisarz itd
  - porównanie odp. między sobą: wspólne fakty, sprzeczności...
  - jeśli odp. są zbyt sprzeczne odrzucamy uczymy model na tym na czym jest pewien
  - można dodać bardzo mądrego sędziego który to oceni
  - trenujemy na tych syntetycznych rekordach

- szkolenie embeddera dla knowledge injection
- próba zaimplementowania dwóch pamięci (pamięć epizodyczna):
  * krótkotrwałe — historia akcji i obserwacji w epizodzie
  * długotrwałe — lista dotychczasowych samo-refleksji (limit przez kontekst)

dataset alfworld
MBP, leetcode hard/gym
commonsense QA — dataset z pytaniami ogólnymi o świecie

- reflexion dla agentów -> react = think + act = sprawia że model nie błądzi i uczy się na swoich przemyśleniach (bierna nagroda)

- aby budować efficient agenta nie trzeba od razu stosować rl lub finetuningu:
  - wykorzystać istniejący LLM jako aktora
  - prosty evaluator (heurystyki, metryki, testy)
  - oddać LLM lub heurystyki jako stabilne LLM
  - przechowywać kilka końcowych refleksji jako pamięć pomocniczą/epizodyczną

- sposób STAR — self taught reflexion:
  - model buduje racje (dobre odp.), jeśli git — zostaje, jeśli nie — hint pomaga i zapis do bazy bez hintu

GPT-J

- generowanie wielu CoT -> problem z tokenami -> potrzebna automatyzacja i pomocnik do wyboru, lepsze generowanie CoT, pomocnik do generowania
- RAG często nie wie jak wyciągać info z baz -> najpierw plan reasoning, potem retrieval
- error taxonomy — self repair -> brak unified listy błędów i repair prompts -> stworzyć unified listę + repair prompts + pamięć błędów aby je uniknąć
- verifier + solver loop -> zbyt ogólne ocenianie errorów oraz ignorowanie feedbacków, mało jasnej walidacji która pokaże lepszą odp-> potrzebna structured verification (correctness, completeness, step validity, hallucinations), kiedy kończymy pętle oeraz użycie poprawionych wag
- dekompozycja reasoning często za dużo pytań -> jakieś ograniczenia, dependency graph, logiczna kolejność zadawanych pytań a nie losowo
- wprowadzić różne role modelu dla augemtnacji i szerszego zakresu semantyki
